{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demucs import pretrained\n",
    "import torch\n",
    "from demucs.demucs import Demucs\n",
    "from demucs.hdemucs import HDemucs\n",
    "from demucs.apply import tensor_chunk\n",
    "from demucs.htdemucs import HTDemucs\n",
    "from demucs.utils import center_trim\n",
    "from demucs.apply import TensorChunk\n",
    "from demucs.audio import AudioFile, convert_audio, save_audio\n",
    "from pathlib import Path\n",
    "import demucs\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "from scipy.signal import resample, butter, filtfilt, cheby1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import sys\n",
    "import io\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from demucs.transformer import MyTransformerEncoderLayer, CrossTransformerEncoderLayer, dynamic_sparse_attention, MultiheadAttention, scaled_dot_product_attention\n",
    "from torch.quantization import quantize_dynamic\n",
    "from fractions import Fraction\n",
    "import kd_helper\n",
    "from demucs.solver import Solver\n",
    "import logging\n",
    "from demucs import distrib\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from dora import hydra_main\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from demucs.separate import Separator\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "separator = Separator(\n",
    "    model=\"htdemucs\",\n",
    "    repo=None,\n",
    "    device=device,\n",
    "    shifts=1,\n",
    "    overlap=0.25,\n",
    "    split=True,\n",
    "    segment=None,\n",
    "    jobs=None,\n",
    "    callback=print\n",
    ")\n",
    "segment = None\n",
    "callback = None\n",
    "length = None\n",
    "samplerate = 44100\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of parameters of a torch model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_htdemucs = pretrained.get_model('htdemucs')\n",
    "model_htdemucs.use_train_segment = False\n",
    "teacher_model = model_htdemucs.models[0]\n",
    "teacher_model.use_train_segment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial weights transferred successfully from the teacher to the student model.\n"
     ]
    }
   ],
   "source": [
    "student_model, teacher_model = kd_helper.get_student_teacher_models(partial_weight_copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41,984,456 parameters in teacher model\n",
      "8,628,760 parameters in student model\n"
     ]
    }
   ],
   "source": [
    "print(f\"{count_parameters(teacher_model):,} parameters in teacher model\")\n",
    "print(f\"{count_parameters(student_model):,} parameters in student model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for teacher model:  3.7313716411590576\n",
      "Teacher model output shape:  torch.Size([1, 4, 2, 44100])\n",
      "Time taken for student model:  0.15351533889770508\n",
      "Student model output shape:  torch.Size([1, 4, 2, 44100])\n"
     ]
    }
   ],
   "source": [
    "audio_input = torch.randn(1, 2, 44100)  # Example input\n",
    "# Forward pass through the model\n",
    "teacher_start = time.time()\n",
    "with torch.no_grad():\n",
    "    teacher_separated_sources = teacher_model(audio_input)\n",
    "teacher_end = time.time()\n",
    "print(\"Time taken for teacher model: \", teacher_end - teacher_start)\n",
    "print(\"Teacher model output shape: \", teacher_separated_sources.shape)\n",
    "student_start = time.time()\n",
    "with torch.no_grad():\n",
    "    student_separated_sources = student_model(audio_input)\n",
    "student_end = time.time()\n",
    "print(\"Time taken for student model: \", student_end - student_start)\n",
    "print(\"Student model output shape: \", student_separated_sources.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(student_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_audio(file, method):\n",
    "    wav = AudioFile(file).read(streams=0, samplerate=samplerate, channels=separator._audio_channels)\n",
    "    original_length = wav.shape[1]\n",
    "    if method[0] is None:\n",
    "        return wav, original_length\n",
    "    elif method[0] == \"decimation_without_filtering\":\n",
    "        decimation_factor = method[1]\n",
    "        wav = wav[:, ::decimation_factor]\n",
    "        return wav, original_length\n",
    "    elif method[0] == \"decimation_with_butterworth_filter\":\n",
    "        cutoff, order, decimation_factor = method[1]\n",
    "        nyquist = 0.5 * samplerate\n",
    "        normal_cutoff = cutoff / nyquist\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        wav = filtfilt(b, a, wav, axis=1)\n",
    "        wav = wav[:, ::decimation_factor]\n",
    "        wav_tensor = torch.tensor(np.copy(wav), dtype=torch.float32)\n",
    "        return wav_tensor, original_length\n",
    "    elif method[0] == \"decimation_with_chebyshev_filter\":\n",
    "        cutoff, order, ripple, decimation_factor = method[1]\n",
    "        nyquist = 0.5 * samplerate\n",
    "        normal_cutoff = cutoff / nyquist\n",
    "        b, a = cheby1(order, ripple, normal_cutoff, btype='low', analog=False)\n",
    "        wav = filtfilt(b, a, wav, axis=1)\n",
    "        wav = wav[:, ::decimation_factor]\n",
    "        wav_tensor = torch.tensor(np.copy(wav), dtype=torch.float32)\n",
    "        return wav_tensor, original_length\n",
    "    assert False, \"Invalid method\"\n",
    "\n",
    "def interpolate_wav_file(wav, original_length):\n",
    "    return resample(wav, original_length, axis=1)\n",
    "\n",
    "def clean_up_out_wav(out, wav, original_length):\n",
    "    wav = torch.tensor(resample(wav, original_length, axis=1))\n",
    "    out = torch.tensor(resample(out, original_length, axis=3))\n",
    "    return out, wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @track_emissions()\n",
    "def run_separator_htdemucs(model, file, output_save_folder = \"random_files\", save_audio_flag=True, method=[None]):\n",
    "    with torch.no_grad():\n",
    "        os.makedirs(output_save_folder, exist_ok=True)\n",
    "        wav, original_length = get_filtered_audio(file, method)\n",
    "        ref = wav.mean(0)\n",
    "        wav -= ref.mean()\n",
    "        wav /= ref.std() + 1e-8\n",
    "        mix = wav[None]\n",
    "        # Assuming the rest of your code remains unchanged\n",
    "        filename_format = \"{stem}.{ext}\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            out = model(mix)\n",
    "        end_time = time.time()\n",
    "\n",
    "        assert isinstance(out, torch.Tensor)\n",
    "        out *= ref.std() + 1e-8\n",
    "        out += ref.mean()\n",
    "        wav *= ref.std() + 1e-8\n",
    "        wav += ref.mean()\n",
    "        out, wav = clean_up_out_wav(out, wav, original_length)\n",
    "        separated = (wav, dict(zip(separator._model.sources, out[0])))[1]\n",
    "        ext = \"mp3\"\n",
    "        kwargs = {\n",
    "            \"samplerate\": samplerate,\n",
    "            \"bitrate\": 320,\n",
    "            \"clip\": \"rescale\",\n",
    "            \"as_float\": False,\n",
    "            \"bits_per_sample\": 16,\n",
    "        }\n",
    "        last_ret = {}\n",
    "        for stem, source in separated.items():\n",
    "            stem_path = os.path.join(output_save_folder, filename_format.format(\n",
    "                stem=stem,\n",
    "                ext=ext,\n",
    "            ))\n",
    "            if save_audio_flag:\n",
    "                save_audio(source, str(stem_path), **kwargs)\n",
    "            else:\n",
    "                last_ret[stem] = source\n",
    "            # loaded_wav, _ = get_filtered_audio(stem_path, [None])\n",
    "            # assert source.shape == loaded_wav.shape, f\"{source.shape} != {loaded_wav.shape}\"\n",
    "        inference_time = end_time - start_time\n",
    "        return inference_time, None, None, last_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9280879497528076, None, None, {})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_separator_htdemucs(teacher_model, \"my_test_short.mp4\", method=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/odml_final/distillation_demucs/MUSDBHQ\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seed = 42\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 2\n",
    "        # Dataset related arguments\n",
    "        self.dset = self.DatasetArgs()\n",
    "        # Optimization related arguments\n",
    "        self.optim = self.OptimArgs()\n",
    "        # Augmentation related arguments\n",
    "        self.augment = self.AugmentArgs()\n",
    "        # Testing related arguments\n",
    "        self.test = self.Test()\n",
    "        # Miscellaneous arguments\n",
    "        self.misc = self.MiscArgs()\n",
    "        self.sources = ['drums', 'bass', 'other', 'vocals']\n",
    "\n",
    "    class DatasetArgs:\n",
    "        def __init__(self):\n",
    "            self.musdb = '/home/ubuntu/odml_final/distillation_demucs/MUSDBHQ'\n",
    "            self.musdb_samplerate = 44100\n",
    "            self.use_musdb = True\n",
    "            self.wav = None  # path to custom wav dataset\n",
    "            self.wav2 = None  # second custom wav dataset\n",
    "            self.segment = 11\n",
    "            self.shift = 1\n",
    "            self.train_valid = False\n",
    "            self.full_cv = True\n",
    "            self.samplerate = 44100\n",
    "            self.channels = 2\n",
    "            self.normalize = True\n",
    "            self.metadata = './metadata'\n",
    "            self.sources = ['drums', 'bass', 'other', 'vocals']\n",
    "            self.valid_samples = None  # valid dataset size\n",
    "            self.backend = None\n",
    "\n",
    "    class OptimArgs:\n",
    "        def __init__(self):\n",
    "            self.lr = 3e-4\n",
    "            self.momentum = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.loss = 'l1'  # l1 or mse\n",
    "            self.optim = 'adam'\n",
    "            self.weight_decay = 0\n",
    "            self.clip_grad = 0\n",
    "\n",
    "    class AugmentArgs:\n",
    "        def __init__(self):\n",
    "            self.shift_same = False\n",
    "            self.repitch = self.Repitch()\n",
    "            self.remix = self.Remix()\n",
    "            self.scale = self.Scale()\n",
    "            self.flip = True\n",
    "\n",
    "        class Repitch:\n",
    "            def __init__(self):\n",
    "                self.proba = 0.2\n",
    "                self.max_tempo = 12\n",
    "\n",
    "        class Remix:\n",
    "            def __init__(self):\n",
    "                self.proba = 1\n",
    "                self.group_size = 4\n",
    "        \n",
    "        class Scale:\n",
    "            def __init__(self):\n",
    "                self.proba = 1\n",
    "                self.min = 0.25\n",
    "                self.max = 1.25\n",
    "        \n",
    "\n",
    "    class Test:\n",
    "        def __init__(self):\n",
    "            self.save = False\n",
    "            self.best = True\n",
    "            self.workers = 2\n",
    "            self.every = 5\n",
    "            self.split = True\n",
    "            self.shifts = 1\n",
    "            self.overlap = 0.25\n",
    "            self.sdr = True\n",
    "            self.metric = 'loss'\n",
    "            self.nonhq = None\n",
    "\n",
    "    class MiscArgs:\n",
    "        def __init__(self):\n",
    "            # You can add any other miscellaneous arguments here if needed.\n",
    "            self.show = False\n",
    "            self.num_workers = 10\n",
    "            self.num_prints = 4\n",
    "            self.verbose = False\n",
    "\n",
    "# Initialize args object with default values from the config file\n",
    "args = Args()\n",
    "\n",
    "# Accessing a parameter would be like this:\n",
    "print(args.dset.musdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demucs.train\n",
    "from kt_solver import KTSolver\n",
    "from demucs.repitch import RepitchedWrapper\n",
    "\n",
    "\n",
    "train_set, valid_set = demucs.train.get_datasets(args)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_my_solver(args, model_only=False):\n",
    "    distrib.init()\n",
    "    torch.manual_seed(args.seed)\n",
    "    teacher_model, student_model = kd_helper.get_student_teacher_models(partial_weight_copy=True)\n",
    "    if args.misc.show:\n",
    "        mb = sum(p.numel() for p in teacher_model.parameters()) * 4 / 2**20\n",
    "        print(f\"Teacher model has {mb:.1f}MB\")\n",
    "        smb = sum(p.numel() for p in student_model.parameters()) * 4 / 2**20\n",
    "        print(f\"Student model has {smb:.1f}MB\")\n",
    "        if hasattr(teacher_model, \"valid_length\"):\n",
    "            field = teacher_model.valid_length(1)\n",
    "            print(f\"Field: {field/args.dset.samplerate*1000:.1f}ms\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    teacher_model.to(device)\n",
    "    student_model.to(device)\n",
    "    \n",
    "    optimizer = demucs.train.get_optimizer(student_model, args)\n",
    "    \n",
    "    assert args.batch_size % distrib.world_size == 0\n",
    "    args.batch_size //= distrib.world_size\n",
    "    \n",
    "    if model_only:\n",
    "        return KTSolver(None, student_model, teacher_model, optimizer, args)\n",
    "    \n",
    "    train_set, valid_set = demucs.train.get_datasets(args)\n",
    "    \n",
    "    if args.augment.repitch.proba:\n",
    "        vocals = []\n",
    "        if 'vocals' in args.dset.sources:\n",
    "            vocals.append(args.sources.index('vocals'))\n",
    "        else:\n",
    "            logger.warning(\"No vocal source found\")\n",
    "        if args.augment.repitch.proba:\n",
    "            train_set = RepitchedWrapper(train_set, vocals=vocals, **args.augment.repitch)\n",
    "    \n",
    "    logger.info(\"train/valid set size: %d %d\", len(train_set), len(valid_set))\n",
    "    train_loader = distrib.loader(\n",
    "        train_set, batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.misc.num_workers, drop_last=True)\n",
    "    if args.dset.full_cv:\n",
    "        valid_loader = distrib.loader(\n",
    "            valid_set, batch_size=1, shuffle=False,\n",
    "            num_workers=args.misc.num_workers)\n",
    "    else:\n",
    "        valid_loader = distrib.loader(\n",
    "            valid_set, batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.misc.num_workers, drop_last=True)\n",
    "    loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
    "\n",
    "    # Construct Solver\n",
    "    return KTSolver(loaders, student_model, teacher_model, optimizer, args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = demucs.train.get_datasets(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 485100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odml-demucs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
