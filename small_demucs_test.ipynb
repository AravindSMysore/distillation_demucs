{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demucs import pretrained\n",
    "import torch\n",
    "from demucs.demucs import Demucs\n",
    "from demucs.hdemucs import HDemucs\n",
    "from demucs.apply import tensor_chunk\n",
    "from demucs.htdemucs import HTDemucs\n",
    "from demucs.utils import center_trim\n",
    "from demucs.apply import TensorChunk\n",
    "from demucs.audio import AudioFile, convert_audio, save_audio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "from scipy.signal import resample, butter, filtfilt, cheby1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "import warnings\n",
    "import sys\n",
    "import io\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from demucs.transformer import MyTransformerEncoderLayer, CrossTransformerEncoderLayer, dynamic_sparse_attention, MultiheadAttention, scaled_dot_product_attention\n",
    "from torch.quantization import quantize_dynamic\n",
    "from fractions import Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from demucs.separate import Separator\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "separator = Separator(\n",
    "    model=\"htdemucs\",\n",
    "    repo=None,\n",
    "    device=device,\n",
    "    shifts=1,\n",
    "    overlap=0.25,\n",
    "    split=True,\n",
    "    segment=None,\n",
    "    jobs=None,\n",
    "    callback=print\n",
    ")\n",
    "segment = None\n",
    "callback = None\n",
    "length = None\n",
    "samplerate = 44100\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of parameters of a torch model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_htdemucs = pretrained.get_model('htdemucs')\n",
    "model_htdemucs.use_train_segment = False\n",
    "teacher_model = model_htdemucs.models[0]\n",
    "teacher_model.use_train_segment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_kwargs = {'sources': ['drums', 'bass', 'other', 'vocals'], 'audio_channels': 2, 'samplerate': 44100, 'segment': Fraction(39, 5), 'channels': 48, 'channels_time': None, 'growth': 2, 'nfft': 4096, 'wiener_iters': 0, 'end_iters': 0, 'wiener_residual': False, 'cac': True, 'depth': 4, 'rewrite': True, 'multi_freqs': [], 'multi_freqs_depth': 3, 'freq_emb': 0.2, 'emb_scale': 10, 'emb_smooth': True, 'kernel_size': 8, 'stride': 4, 'time_stride': 2, 'context': 1, 'context_enc': 0, 'norm_starts': 4, 'norm_groups': 4, 'dconv_mode': 3, 'dconv_depth': 2, 'dconv_comp': 8, 'dconv_init': 0.001, 'bottom_channels': 512, 't_layers': 5, 't_hidden_scale': 4.0, 't_heads': 8, 't_dropout': 0.02, 't_layer_scale': True, 't_gelu': True, 't_emb': 'sin', 't_max_positions': 10000, 't_max_period': 10000.0, 't_weight_pos_embed': 1.0, 't_cape_mean_normalize': True, 't_cape_augment': True, 't_cape_glob_loc_scale': [5000.0, 1.0, 1.4], 't_sin_random_shift': 0, 't_norm_in': True, 't_norm_in_group': False, 't_group_norm': False, 't_norm_first': True, 't_norm_out': True, 't_weight_decay': 0.0, 't_lr': None, 't_sparse_self_attn': False, 't_sparse_cross_attn': False, 't_mask_type': 'diag', 't_mask_random_seed': 42, 't_sparse_attn_window': 400, 't_global_window': 100, 't_sparsity': 0.95, 't_auto_sparsity': False, 't_cross_first': False, 'rescale': 0.1}\n",
    "\n",
    "student_kwargs = {k: v for k, v in teacher_kwargs.items()}\n",
    "student_kwargs['channels'] = 12 # 48\n",
    "# student_kwargs['depth'] = 2 # 4\n",
    "# student_kwargs['kernel_size'] = 4 # 8\n",
    "student_kwargs['time_stride'] = 2 # 2\n",
    "# student_kwargs['stride'] = 2 # 4\n",
    "student_kwargs['t_layers'] = 5 # 5\n",
    "# student_kwargs['t_heads'] = 4 # 8\n",
    "student_model = HTDemucs(**student_kwargs)\n",
    "student_model.use_train_segment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41,984,456 parameters in teacher model\n",
      "32,367,128 parameters in student model\n"
     ]
    }
   ],
   "source": [
    "print(f\"{count_parameters(teacher_model):,} parameters in teacher model\")\n",
    "print(f\"{count_parameters(student_model):,} parameters in student model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for teacher model:  0.40648746490478516\n",
      "Teacher model output shape:  torch.Size([1, 4, 2, 44100])\n",
      "Time taken for student model:  0.25122547149658203\n",
      "Student model output shape:  torch.Size([1, 4, 2, 44100])\n"
     ]
    }
   ],
   "source": [
    "audio_input = torch.randn(1, 2, 44100)  # Example input\n",
    "# Forward pass through the model\n",
    "teacher_start = time.time()\n",
    "with torch.no_grad():\n",
    "    teacher_separated_sources = teacher_model(audio_input)\n",
    "teacher_end = time.time()\n",
    "print(\"Time taken for teacher model: \", teacher_end - teacher_start)\n",
    "print(\"Teacher model output shape: \", teacher_separated_sources.shape)\n",
    "student_start = time.time()\n",
    "with torch.no_grad():\n",
    "    student_separated_sources = student_model(audio_input)\n",
    "student_end = time.time()\n",
    "print(\"Time taken for student model: \", student_end - student_start)\n",
    "print(\"Student model output shape: \", student_separated_sources.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41,984,456 parameters in teacher model\n"
     ]
    }
   ],
   "source": [
    "print(f\"{count_parameters(teacher_model):,} parameters in teacher model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_audio(file, method):\n",
    "    wav = AudioFile(file).read(streams=0, samplerate=samplerate, channels=separator._audio_channels)\n",
    "    original_length = wav.shape[1]\n",
    "    if method[0] is None:\n",
    "        return wav, original_length\n",
    "    elif method[0] == \"decimation_without_filtering\":\n",
    "        decimation_factor = method[1]\n",
    "        wav = wav[:, ::decimation_factor]\n",
    "        return wav, original_length\n",
    "    elif method[0] == \"decimation_with_butterworth_filter\":\n",
    "        cutoff, order, decimation_factor = method[1]\n",
    "        nyquist = 0.5 * samplerate\n",
    "        normal_cutoff = cutoff / nyquist\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        wav = filtfilt(b, a, wav, axis=1)\n",
    "        wav = wav[:, ::decimation_factor]\n",
    "        wav_tensor = torch.tensor(np.copy(wav), dtype=torch.float32)\n",
    "        return wav_tensor, original_length\n",
    "    elif method[0] == \"decimation_with_chebyshev_filter\":\n",
    "        cutoff, order, ripple, decimation_factor = method[1]\n",
    "        nyquist = 0.5 * samplerate\n",
    "        normal_cutoff = cutoff / nyquist\n",
    "        b, a = cheby1(order, ripple, normal_cutoff, btype='low', analog=False)\n",
    "        wav = filtfilt(b, a, wav, axis=1)\n",
    "        wav = wav[:, ::decimation_factor]\n",
    "        wav_tensor = torch.tensor(np.copy(wav), dtype=torch.float32)\n",
    "        return wav_tensor, original_length\n",
    "    assert False, \"Invalid method\"\n",
    "\n",
    "def interpolate_wav_file(wav, original_length):\n",
    "    return resample(wav, original_length, axis=1)\n",
    "\n",
    "def clean_up_out_wav(out, wav, original_length):\n",
    "    wav = torch.tensor(resample(wav, original_length, axis=1))\n",
    "    out = torch.tensor(resample(out, original_length, axis=3))\n",
    "    return out, wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @track_emissions()\n",
    "def run_separator_htdemucs(model, file, output_save_folder = \"random_files\", save_audio_flag=True, method=[None]):\n",
    "    with torch.no_grad():\n",
    "        os.makedirs(output_save_folder, exist_ok=True)\n",
    "        wav, original_length = get_filtered_audio(file, method)\n",
    "        ref = wav.mean(0)\n",
    "        wav -= ref.mean()\n",
    "        wav /= ref.std() + 1e-8\n",
    "        mix = wav[None]\n",
    "        # Assuming the rest of your code remains unchanged\n",
    "        filename_format = \"{stem}.{ext}\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            out = model(mix)\n",
    "        end_time = time.time()\n",
    "\n",
    "        assert isinstance(out, torch.Tensor)\n",
    "        out *= ref.std() + 1e-8\n",
    "        out += ref.mean()\n",
    "        wav *= ref.std() + 1e-8\n",
    "        wav += ref.mean()\n",
    "        out, wav = clean_up_out_wav(out, wav, original_length)\n",
    "        separated = (wav, dict(zip(separator._model.sources, out[0])))[1]\n",
    "        ext = \"mp3\"\n",
    "        kwargs = {\n",
    "            \"samplerate\": samplerate,\n",
    "            \"bitrate\": 320,\n",
    "            \"clip\": \"rescale\",\n",
    "            \"as_float\": False,\n",
    "            \"bits_per_sample\": 16,\n",
    "        }\n",
    "        last_ret = {}\n",
    "        for stem, source in separated.items():\n",
    "            stem_path = os.path.join(output_save_folder, filename_format.format(\n",
    "                stem=stem,\n",
    "                ext=ext,\n",
    "            ))\n",
    "            if save_audio_flag:\n",
    "                save_audio(source, str(stem_path), **kwargs)\n",
    "            else:\n",
    "                last_ret[stem] = source\n",
    "            # loaded_wav, _ = get_filtered_audio(stem_path, [None])\n",
    "            # assert source.shape == loaded_wav.shape, f\"{source.shape} != {loaded_wav.shape}\"\n",
    "        inference_time = end_time - start_time\n",
    "        return inference_time, None, None, last_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_filtered_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_separator_htdemucs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_test_short.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m, in \u001b[0;36mrun_separator_htdemucs\u001b[1;34m(model, file, output_save_folder, save_audio_flag, method)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_save_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m     wav, original_length \u001b[38;5;241m=\u001b[39m \u001b[43mget_filtered_audio\u001b[49m(file, method)\n\u001b[0;32m      6\u001b[0m     ref \u001b[38;5;241m=\u001b[39m wav\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m     wav \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m ref\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_filtered_audio' is not defined"
     ]
    }
   ],
   "source": [
    "run_separator_htdemucs(teacher_model, \"my_test_short.mp4\", method=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odml-demucs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
