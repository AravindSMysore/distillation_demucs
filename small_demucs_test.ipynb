{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demucs import pretrained\n",
    "import torch\n",
    "from demucs.demucs import Demucs\n",
    "from demucs.hdemucs import HDemucs\n",
    "from demucs.apply import tensor_chunk\n",
    "from demucs.htdemucs import HTDemucs\n",
    "from demucs.utils import center_trim\n",
    "from demucs.apply import TensorChunk\n",
    "from demucs.audio import AudioFile, convert_audio, save_audio\n",
    "from pathlib import Path\n",
    "import demucs\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "from scipy.signal import resample, butter, filtfilt, cheby1\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import sys\n",
    "import io\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from demucs.transformer import MyTransformerEncoderLayer, CrossTransformerEncoderLayer, dynamic_sparse_attention, MultiheadAttention, scaled_dot_product_attention\n",
    "from torch.quantization import quantize_dynamic\n",
    "from fractions import Fraction\n",
    "import kd_helper\n",
    "from demucs.solver import Solver\n",
    "import logging\n",
    "from demucs import distrib\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from dora import hydra_main\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from demucs.separate import Separator\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "separator = Separator(\n",
    "    model=\"htdemucs\",\n",
    "    repo=None,\n",
    "    device=device,\n",
    "    shifts=1,\n",
    "    overlap=0.25,\n",
    "    split=True,\n",
    "    segment=None,\n",
    "    jobs=None,\n",
    "    callback=print\n",
    ")\n",
    "segment = None\n",
    "callback = None\n",
    "length = None\n",
    "samplerate = 44100\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of parameters of a torch model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial weights transferred successfully from the teacher to the student model.\n"
     ]
    }
   ],
   "source": [
    "student_model, teacher_model = kd_helper.get_student_teacher_models(partial_weight_copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41,984,456 parameters in teacher model\n",
      "8,628,760 parameters in student model\n"
     ]
    }
   ],
   "source": [
    "print(f\"{count_parameters(teacher_model):,} parameters in teacher model\")\n",
    "print(f\"{count_parameters(student_model):,} parameters in student model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for teacher model:  3.187408924102783\n",
      "Teacher model output shape:  torch.Size([1, 4, 2, 220500])\n",
      "Time taken for student model:  1.1205344200134277\n",
      "Student model output shape:  torch.Size([1, 4, 2, 220500])\n"
     ]
    }
   ],
   "source": [
    "audio_input = torch.randn(1, 2, 44100*5)  # Example input\n",
    "# Forward pass through the model\n",
    "teacher_start = time.time()\n",
    "with torch.no_grad():\n",
    "    teacher_separated_sources = teacher_model(audio_input)\n",
    "teacher_end = time.time()\n",
    "print(\"Time taken for teacher model: \", teacher_end - teacher_start)\n",
    "print(\"Teacher model output shape: \", teacher_separated_sources.shape)\n",
    "student_start = time.time()\n",
    "with torch.no_grad():\n",
    "    # student_separated_sources = student_model(audio_input[:, :, ::2])\n",
    "    student_separated_sources = student_model(audio_input)\n",
    "student_end = time.time()\n",
    "print(\"Time taken for student model: \", student_end - student_start)\n",
    "print(\"Student model output shape: \", student_separated_sources.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/distillation_demucs/musdb18hq\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seed = 42\n",
    "        self.batch_size = 8\n",
    "        self.epochs = 50\n",
    "        self.downsample_factor = 2\n",
    "        # Dataset related arguments\n",
    "        self.dset = self.DatasetArgs()\n",
    "        # Optimization related arguments\n",
    "        self.optim = self.OptimArgs()\n",
    "        # Augmentation related arguments\n",
    "        self.augment = self.AugmentArgs()\n",
    "        # Testing related arguments\n",
    "        self.test = self.Test()\n",
    "        # Miscellaneous arguments\n",
    "        self.misc = self.MiscArgs()\n",
    "        self.sources = ['drums', 'bass', 'other', 'vocals']\n",
    "        self.sequence_length_in_seconds = 5\n",
    "        self.save_folder = \"MyTrainingOutputV3/\"\n",
    "\n",
    "    class DatasetArgs:\n",
    "        def __init__(self):\n",
    "            self.musdb = r'/home/ubuntu/distillation_demucs/musdb18hq'\n",
    "            self.musdb_samplerate = 44100\n",
    "            self.use_musdb = True\n",
    "            self.wav = None  # path to custom wav dataset\n",
    "            self.wav2 = None  # second custom wav dataset\n",
    "            self.segment = 11\n",
    "            self.shift = 1\n",
    "            self.train_valid = False\n",
    "            self.full_cv = True\n",
    "            self.samplerate = 44100\n",
    "            self.channels = 2\n",
    "            self.normalize = True\n",
    "            self.metadata = './metadata'\n",
    "            self.sources = ['drums', 'bass', 'other', 'vocals']\n",
    "            self.valid_samples = None  # valid dataset size\n",
    "            self.backend = None\n",
    "\n",
    "    class OptimArgs:\n",
    "        def __init__(self):\n",
    "            self.lr = 3e-4\n",
    "            self.momentum = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.loss = 'l1'  # l1 or mse\n",
    "            self.optim = 'adam'\n",
    "            self.weight_decay = 0\n",
    "            self.clip_grad = 0\n",
    "\n",
    "    class AugmentArgs:\n",
    "        def __init__(self):\n",
    "            self.shift_same = False\n",
    "            self.repitch = self.Repitch()\n",
    "            self.remix = self.Remix()\n",
    "            self.scale = self.Scale()\n",
    "            self.flip = True\n",
    "\n",
    "        class Repitch:\n",
    "            def __init__(self):\n",
    "                self.proba = 0.2\n",
    "                self.max_tempo = 12\n",
    "\n",
    "        class Remix:\n",
    "            def __init__(self):\n",
    "                self.proba = 1\n",
    "                self.group_size = 4\n",
    "        \n",
    "        class Scale:\n",
    "            def __init__(self):\n",
    "                self.proba = 1\n",
    "                self.min = 0.25\n",
    "                self.max = 1.25\n",
    "\n",
    "    class Test:\n",
    "        def __init__(self):\n",
    "            self.save = False\n",
    "            self.best = True\n",
    "            self.workers = 2\n",
    "            self.every = 5\n",
    "            self.split = True\n",
    "            self.shifts = 1\n",
    "            self.overlap = 0.25\n",
    "            self.sdr = True\n",
    "            self.metric = 'loss'\n",
    "            self.nonhq = None\n",
    "\n",
    "    class MiscArgs:\n",
    "        def __init__(self):\n",
    "            # You can add any other miscellaneous arguments here if needed.\n",
    "            self.show = False\n",
    "            self.num_workers = 6\n",
    "            self.num_prints = 4\n",
    "            self.verbose = False\n",
    "\n",
    "# Initialize args object with default values from the config file\n",
    "args = Args()\n",
    "\n",
    "# Accessing a parameter would be like this:\n",
    "print(args.dset.musdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demucs.train\n",
    "from kt_solver import KTSolver\n",
    "from demucs.repitch import RepitchedWrapper\n",
    "\n",
    "\n",
    "# train_set, valid_set = demucs.train.get_datasets(args)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "distrib.init()\n",
    "\n",
    "def get_my_solver(args, model_only=False):\n",
    "    torch.manual_seed(args.seed)\n",
    "    teacher_model, student_model = kd_helper.get_student_teacher_models(partial_weight_copy=True)\n",
    "    if args.misc.show:\n",
    "        mb = sum(p.numel() for p in teacher_model.parameters()) * 4 / 2**20\n",
    "        print(f\"Teacher model has {mb:.1f}MB\")\n",
    "        smb = sum(p.numel() for p in student_model.parameters()) * 4 / 2**20\n",
    "        print(f\"Student model has {smb:.1f}MB\")\n",
    "        if hasattr(teacher_model, \"valid_length\"):\n",
    "            field = teacher_model.valid_length(1)\n",
    "            print(f\"Field: {field/args.dset.samplerate*1000:.1f}ms\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    teacher_model.to(device)\n",
    "    student_model.to(device)\n",
    "    \n",
    "    optimizer = demucs.train.get_optimizer(student_model, args)\n",
    "    \n",
    "    assert args.batch_size % distrib.world_size == 0\n",
    "    args.batch_size //= distrib.world_size\n",
    "    \n",
    "    if model_only:\n",
    "        return KTSolver(None, student_model, teacher_model, optimizer, args)\n",
    "    \n",
    "    train_set, valid_set = demucs.train.get_datasets(args)\n",
    "    \n",
    "    if args.augment.repitch.proba:\n",
    "        vocals = []\n",
    "        if 'vocals' in args.dset.sources:\n",
    "            vocals.append(args.sources.index('vocals'))\n",
    "        else:\n",
    "            logger.warning(\"No vocal source found\")\n",
    "        if args.augment.repitch.proba:\n",
    "            train_set = RepitchedWrapper(train_set, vocals=vocals, **args.augment.repitch)\n",
    "\n",
    "    logger.info(\"train/valid set size: %d %d\", len(train_set), len(valid_set))\n",
    "    train_loader = distrib.loader(\n",
    "        train_set, batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.misc.num_workers, drop_last=True)\n",
    "    if args.dset.full_cv:\n",
    "        valid_loader = distrib.loader(\n",
    "            valid_set, batch_size=1, shuffle=False,\n",
    "            num_workers=args.misc.num_workers)\n",
    "    else:\n",
    "        valid_loader = distrib.loader(\n",
    "            valid_set, batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.misc.num_workers, drop_last=True)\n",
    "    loaders = {\"train\": train_loader, \"valid\": valid_loader}\n",
    "\n",
    "    # Construct Solver\n",
    "    return KTSolver(loaders, student_model, teacher_model, optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = demucs.train.get_datasets(args)\n",
    "train_loader = distrib.loader(\n",
    "    train_set, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.misc.num_workers, drop_last=True)\n",
    "valid_loader = distrib.loader(\n",
    "    valid_set, batch_size=1, shuffle=False,\n",
    "    num_workers=args.misc.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_downsampling(wav_batch, downsample_factor):\n",
    "    assert len(wav_batch.shape) == 4, f\"Expected 4D tensor, got {len(wav_batch.shape)}D tensor\"\n",
    "    return wav_batch[:, :, :, ::downsample_factor]\n",
    "\n",
    "def get_random_slice(wav_batch, segment_length_in_seconds, seed=None):\n",
    "    bs, num_sources, channels, num_timesteps = wav_batch.shape\n",
    "    total_length = args.dset.samplerate * segment_length_in_seconds\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    if num_timesteps <= total_length:\n",
    "        return wav_batch\n",
    "    random_start = np.random.randint(0, num_timesteps - total_length)\n",
    "    ret = wav_batch[:, :, :, random_start:random_start + total_length]\n",
    "    assert ret.shape == (bs, num_sources, channels, total_length), f\"Expected shape {(bs, num_sources, channels, total_length)}, got {ret.shape}\"\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_names = args.sources\n",
    "def calculate_sdr(target: torch.Tensor, estimate: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Signal-to-Distortion Ratio (SDR).\n",
    "\n",
    "    Args:\n",
    "        target (torch.Tensor): The ground truth signal of shape (2, timesteps).\n",
    "        estimate (torch.Tensor): The estimated signal of shape (2, timesteps).\n",
    "\n",
    "    Returns:\n",
    "        float: The SDR value in decibels.\n",
    "    \"\"\"\n",
    "    # Ensure the input tensors are of the same shape\n",
    "    assert target.shape == estimate.shape, \"Target and estimate must have the same shape.\"\n",
    "    target_norm_squared = torch.norm(target, p=2) ** 2\n",
    "\n",
    "    error = target - estimate\n",
    "    error_norm_squared = torch.norm(error, p=2) ** 2\n",
    "\n",
    "    sdr = 10 * torch.log10(target_norm_squared / error_norm_squared)\n",
    "    return sdr.item()\n",
    "\n",
    "def calculate_sdr_stem(target_list, model_pred_list):\n",
    "    sdr_list = []\n",
    "    for target, model_pred in zip(target_list, model_pred_list):\n",
    "        sdr_list.append(calculate_sdr(target, model_pred))\n",
    "    return np.array(sdr_list).mean()\n",
    "\n",
    "def get_sdr_all(actual_audio, model_audio):\n",
    "    sdr_list = []\n",
    "    for split in splits_names:\n",
    "        sdr_list.append(calculate_sdr_stem(actual_audio[split], model_audio[split]))\n",
    "    return np.array(sdr_list).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audios_from_batch(sources_batch, estimates_batch):\n",
    "    actual_audio = {i: [] for i in splits_names}\n",
    "    model_audio = {i: [] for i in splits_names}\n",
    "    assert sources_batch.shape == estimates_batch.shape, f\"Expected {sources_batch.shape} == {estimates_batch.shape}\"\n",
    "    bs, num_sources, num_channels, num_samples = sources_batch.shape\n",
    "    assert num_sources == 4, f\"Expected 4 sources, got {num_sources}\"\n",
    "    for ind, split in enumerate(splits_names):\n",
    "        for i in range(bs):\n",
    "            actual_audio[split].append(sources_batch[i, ind])\n",
    "            model_audio[split].append(estimates_batch[i, ind])\n",
    "    return actual_audio, model_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demucs.apply import apply_model\n",
    "def evaluate(model, loader, is_student_model=False):\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_actual_audio = {i: [] for i in splits_names}\n",
    "        total_model_audio = {i: [] for i in splits_names}\n",
    "        \n",
    "        for ind, sources in enumerate(tqdm(loader)):\n",
    "            sources = sources.to(device)\n",
    "            bs, num_sources, channels, num_timesteps = sources.shape\n",
    "            sources = get_random_slice(sources, args.sequence_length_in_seconds, seed=ind)\n",
    "            if is_student_model:\n",
    "                sources = apply_downsampling(sources, args.downsample_factor)\n",
    "            if num_sources == 5:\n",
    "                mix = sources[:, 0]\n",
    "                sources = sources[:, 1:]\n",
    "                estimates = apply_model(model, mix, split=args.test.split, overlap=0)\n",
    "                actual_audio, model_audio = get_audios_from_batch(sources, estimates)\n",
    "            elif num_sources == 4:\n",
    "                mix = sources.sum(dim=1)\n",
    "                estimates = apply_model(model, mix, split=args.test.split, overlap=0)\n",
    "                actual_audio, model_audio = get_audios_from_batch(sources, estimates)\n",
    "            else:\n",
    "                assert False, f\"Expected 4 or 5 sources, got {num_sources}\"\n",
    "            \n",
    "            for split in splits_names:\n",
    "                total_actual_audio[split].extend(actual_audio[split])\n",
    "                total_model_audio[split].extend(model_audio[split])\n",
    "        sdr = get_sdr_all(total_actual_audio, total_model_audio)\n",
    "        return sdr\n",
    "# evaluate(teacher_model, valid_loader, is_student_model=False)\n",
    "# evaluate(student_model, valid_loader, is_student_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_optimizer(model):\n",
    "    my_optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=args.optim.lr,\n",
    "            betas=(args.optim.momentum, args.optim.beta2),\n",
    "            weight_decay=args.optim.weight_decay,\n",
    "        )\n",
    "    return my_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_model(student_model, args, epoch_num):\n",
    "    save_folder = args.save_folder\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    torch.save(student_model.state_dict(), os.path.join(save_folder, f\"student_model_epoch{epoch_num}.pth\"))\n",
    "    print(f\"Saved student model to {save_folder}\")\n",
    "\n",
    "def kd_train(teacher_model, student_model, train_loader, valid_loader, args, debug=False):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "    if debug:\n",
    "        print(\"Student model and teacher model moved to device\")\n",
    "    optimizer = get_my_optimizer(student_model)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Starting training loop\")\n",
    "    for epoch in range(args.epochs):\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "        student_model.train()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.epochs}\", unit=\"batch\")\n",
    "\n",
    "        for ind, sources in enumerate(progress_bar):\n",
    "            if debug:\n",
    "                print(f\"Starting batch {ind+1}\")\n",
    "            sources = sources.to(device)\n",
    "            bs, num_sources, channels, num_timesteps = sources.shape\n",
    "            sources = get_random_slice(sources, args.sequence_length_in_seconds)\n",
    "            og_mix = sources.sum(dim=1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_estimates = apply_model(teacher_model, og_mix, split=args.test.split, overlap=0)\n",
    "                assert teacher_estimates.shape == sources.shape, f\"Expected {teacher_estimates.shape} == {sources.shape}\"\n",
    "            if debug:\n",
    "                print(f\"Teacher estimates shape: {teacher_estimates.shape}\")\n",
    "\n",
    "            sources = apply_downsampling(sources, args.downsample_factor)\n",
    "            mix = sources.sum(dim=1)\n",
    "            teacher_estimates = apply_downsampling(teacher_estimates, args.downsample_factor)\n",
    "\n",
    "            # Forward pass for student model\n",
    "            if debug:\n",
    "                print(f\"Mix shape: {mix.shape}\")\n",
    "                print(\"Starting forward pass for student model\")\n",
    "            student_estimates = student_model(mix)\n",
    "            if debug:\n",
    "                print(f\"Student estimates shape: {student_estimates.shape}\")\n",
    "            \n",
    "            assert student_estimates.shape == sources.shape, f\"Expected {student_estimates.shape} == {sources.shape}\"\n",
    "\n",
    "            # Compute loss\n",
    "            if debug:\n",
    "                print(\"Computing loss\")\n",
    "            loss = criterion(student_estimates, teacher_estimates)\n",
    "            if debug:\n",
    "                print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "            # Backpropagation and optimization step\n",
    "            if debug:\n",
    "                print(\"Backpropagation and optimization step\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if debug:\n",
    "                print(\"Backpropagation and optimization step done\")\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Update progress bar with running average loss\n",
    "            avg_loss = running_loss / num_batches\n",
    "            progress_bar.set_postfix(loss=avg_loss)\n",
    "            if debug:\n",
    "                print(f\"Batch {ind+1} completed\")\n",
    "                return\n",
    "\n",
    "        # Evaluate on validation set and report SDR\n",
    "        valid_sdr = evaluate(student_model, valid_loader, is_student_model=True)\n",
    "        print(f\"Validation SDR: {valid_sdr:.2f}\")\n",
    "\n",
    "        # Save model at the end of each epoch\n",
    "        save_model(student_model, args, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded student model from MyTrainingOutputV3/student_model_epoch4.pth\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"MyTrainingOutputV3/student_model_epoch4.pth\"\n",
    "if checkpoint_path is not None:\n",
    "    student_model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded student model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:10<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial student model SDR: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "initial_sdr = evaluate(student_model, valid_loader, is_student_model=True)\n",
    "print(f\"Initial student model SDR: {initial_sdr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_train(teacher_model, student_model, train_loader, valid_loader, args, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved student model to MyTrainingOutput/\n"
     ]
    }
   ],
   "source": [
    "save_model(student_model, args, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model size: 160.16MB\n",
      "Student model size: 32.92MB\n"
     ]
    }
   ],
   "source": [
    "# Function to get model size in MB\n",
    "def get_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters()) * 4 / 2**20\n",
    "\n",
    "print(f\"Teacher model size: {get_model_size(teacher_model):.2f}MB\")\n",
    "print(f\"Student model size: {get_model_size(student_model):.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odml-demucs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
